{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_anticipation1",
      "provenance": [],
      "collapsed_sections": [
        "gVV7NiO4gzjO",
        "PVvCiGxcfYAU",
        "BcZMRRo1io6D",
        "vQ0MXXUtwoNC",
        "nN2ke3vzyxcf",
        "RzoSSIvhzB1n",
        "yST4UMS5zqN7",
        "bdkZiqi0zvEJ",
        "KiHf_vhwzyX2",
        "67TzuRNuz4HL",
        "4aOGVSoF0POT",
        "MCIzvHNwCEE0",
        "CH7yRqee1SgM",
        "SanHbKcg1WJY",
        "Ubu8U-QH4OQ0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8FM26FAr5Xq"
      },
      "source": [
        "# Dataset: [Breakfast](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "# tensorflow, transformer\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTBzdGaFYg4n"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "import os\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgc4U0s6C7ND"
      },
      "source": [
        "# From annotation work\r\n",
        "- Read Breakfast Dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egp5zhcrC6kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd31c6fe-e21d-4a6e-d5e2-d5edb0dd3adc"
      },
      "source": [
        "# read the dataset and some exploration\r\n",
        "\r\n",
        "g = os.walk(r\"/content/drive/MyDrive/FurtherStudy/Project2020/groundTruthlabel-breakfastdatset/\")\r\n",
        "# g is a generator with (path,dir_list,file_list)\r\n",
        "# https://blog.csdn.net/mighty13/article/details/77995857\r\n",
        "\r\n",
        "file_list = list(g)  # path,dir_list,file_list in g\r\n",
        "path, _, file_list = file_list[0]\r\n",
        "\r\n",
        "fps = 15\r\n",
        "resolution = (320, 240)  # 320 * 240\r\n",
        "# 52 different individuals, 18 different kitchens, 10 cooking activities\r\n",
        "person = {}  \r\n",
        "cams = {}  \r\n",
        "activity = {} \r\n",
        "data_files = []  # txt data file names\r\n",
        "\r\n",
        "for file_name in file_list:\r\n",
        "  if file_name.endswith(\".txt\"):\r\n",
        "    data_files.append(file_name)\r\n",
        "    pts = file_name[:file_name.rfind('.')].split('_')\r\n",
        "    person[pts[0]] = person[pts[0]] + 1 if pts[0] in person else 1\r\n",
        "    cams[pts[1]] = cams[pts[1]] + 1 if pts[1] in cams else 1\r\n",
        "    activity[pts[3]] = activity[pts[3]] + 1 if pts[3] in activity else 1\r\n",
        "\r\n",
        "print(\"Individuals:\", person)\r\n",
        "print(\"Camera:\", cams)\r\n",
        "print(\"Activities:\", activity)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Individuals: {'P51': 46, 'P30': 23, 'P43': 40, 'P14': 29, 'P07': 18, 'P36': 37, 'P13': 23, 'P29': 25, 'P32': 25, 'P23': 44, 'P37': 40, 'P49': 46, 'P27': 35, 'P35': 32, 'P41': 50, 'P20': 39, 'P21': 40, 'P04': 30, 'P17': 34, 'P24': 42, 'P52': 45, 'P16': 37, 'P18': 33, 'P53': 43, 'P42': 48, 'P46': 44, 'P31': 23, 'P47': 41, 'P26': 31, 'P38': 40, 'P22': 41, 'P19': 32, 'P12': 20, 'P39': 43, 'P11': 19, 'P54': 44, 'P15': 29, 'P25': 33, 'P28': 10, 'P05': 11, 'P33': 24, 'P44': 45, 'P48': 42, 'P50': 48, 'P34': 25, 'P40': 46, 'P03': 26, 'P45': 44, 'P06': 11, 'P08': 12, 'P09': 15, 'P10': 9}\n",
            "Camera: {'webcam01': 365, 'cam02': 272, 'webcam02': 338, 'stereo01': 304, 'cam01': 433}\n",
            "Activities: {'sandwich': 169, 'friedegg': 173, 'cereals': 184, 'scrambledegg': 166, 'pancake': 157, 'salat': 163, 'tea': 184, 'milk': 187, 'juice': 162, 'coffee': 167}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKGuaTLaFrxi"
      },
      "source": [
        "- Convert into Video Stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtZjV7o6E7aW"
      },
      "source": [
        "# Video Stream Class\r\n",
        "class VideoStream:\r\n",
        "  def __init__(self, stream=None):\r\n",
        "    self.stream = stream\r\n",
        "    self.total_frame = 0\r\n",
        "    if stream is not None:\r\n",
        "      for _, count in stream:\r\n",
        "        self.total_frame += count\r\n",
        "\r\n",
        "\r\n",
        "class VideoStream1(VideoStream):\r\n",
        "  \"\"\"Video stream as a list of action and frame count\r\n",
        "  Designed for files like those txt files of the breakfast dataset\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, path=None, file_name=None, fps=15, remove=[\"SIL\"],stream=None):\r\n",
        "    self.fps = fps  # frame rate of the original videos\r\n",
        "    self.remove = remove\r\n",
        "    self.file_name = file_name  \r\n",
        "    super(VideoStream1, self).__init__(stream)\r\n",
        "    if path is not None and file_name is not None:\r\n",
        "      stream = []\r\n",
        "      f = open(path+file_name, 'r')\r\n",
        "      temp = \"\"\r\n",
        "      count = 0\r\n",
        "      for line in f.readlines():\r\n",
        "        line = line.strip()\r\n",
        "        if line == temp:\r\n",
        "          count += 1\r\n",
        "        elif line not in remove:\r\n",
        "          if count > 0:\r\n",
        "            stream.append((temp, count))\r\n",
        "            self.total_frame += count\r\n",
        "          temp = line\r\n",
        "          count = 1\r\n",
        "      if temp not in remove and count > 0:\r\n",
        "        stream.append((temp, count))\r\n",
        "        self.total_frame += count\r\n",
        "      self.stream = stream  # a list of (action, frame count)\r\n",
        "      f.close()\r\n",
        "\r\n",
        "  @property\r\n",
        "  def jsonName(self):\r\n",
        "    \"\"\"return json information from file name, including activity/purpose\"\"\"\r\n",
        "    if self.file_name is None:\r\n",
        "      return None\r\n",
        "    file_name = self.file_name\r\n",
        "    pts = file_name[:file_name.rfind('.')].split('_')\r\n",
        "    return {\"file_name\":file_name, \"person\":pts[0], \"camera\":pts[1], \"activity\":pts[3]}\r\n",
        "\r\n",
        "  def obs_by_frame_index(self, end, start=0):\r\n",
        "    \"\"\"return the observation stream between the two given index of frame\r\n",
        "    [start, end)\r\n",
        "    \"\"\"\r\n",
        "    obs = []\r\n",
        "    remain_start = start\r\n",
        "    remain_end = end\r\n",
        "    for action, count in self.stream:\r\n",
        "      if remain_end == 0:\r\n",
        "        return obs\r\n",
        "      if count <= remain_start:\r\n",
        "        remain_start -= count\r\n",
        "        remain_end -= count\r\n",
        "      else:\r\n",
        "        if remain_start > 0:\r\n",
        "          count -= remain_start\r\n",
        "          remain_end -= remain_start\r\n",
        "          remain_start = 0\r\n",
        "        if count > remain_end:\r\n",
        "          obs.append((action, remain_end))\r\n",
        "          return obs\r\n",
        "        else:\r\n",
        "          remain_end -= count\r\n",
        "          obs.append((action, count))\r\n",
        "    return obs\r\n",
        "\r\n",
        "  def action_by_frame_index(self, index):\r\n",
        "    \"\"\"return the action at the given index of frame; return None if the index\r\n",
        "    is too large to stay in range\r\n",
        "    \"\"\"\r\n",
        "    remain = index\r\n",
        "    action_index = 0\r\n",
        "    for action, count in self.stream:\r\n",
        "      if count > remain:\r\n",
        "        return action, action_index\r\n",
        "      else:\r\n",
        "        remain -= count\r\n",
        "        action_index += 1\r\n",
        "    return None, len(self.stream)\r\n",
        "    \r\n",
        "  def label_obs(self, sps, ts):\r\n",
        "    \"\"\" take the first sps part of the data as observations and label the action\r\n",
        "     at t seconds after an observation ends\r\n",
        "\r\n",
        "    split: partial percentage of a video as observation\r\n",
        "    ts: sec after the observation that we interested\r\n",
        "\r\n",
        "    return: json of observations and their labels\r\n",
        "    \"\"\"\r\n",
        "    n = self.total_frame\r\n",
        "    res = {}\r\n",
        "    for p in sps:\r\n",
        "      n_obs = int(np.round(n*p))\r\n",
        "      obs = self.obs_by_frame_index(n_obs)\r\n",
        "      action_label = []\r\n",
        "      for t in ts:\r\n",
        "        frame = n_obs + int(self.fps*t)\r\n",
        "        action_label.append(self.action_by_frame_index(frame-1))\r\n",
        "      res[p] = {\"observation\":obs, \"tsec\":ts, \"action_labels\":action_label}\r\n",
        "    return res\r\n",
        "\r\n",
        "\r\n",
        "class VideoStream2(VideoStream1):\r\n",
        "  def label_obs(self, ts, step, window_size, unit_is_second=True):\r\n",
        "    \"\"\"use sliding window instead of ratio\r\n",
        "    unit: second if unit_is_second==True else frame\r\n",
        "    \"\"\"\r\n",
        "    fps = self.fps\r\n",
        "    if unit_is_second:\r\n",
        "      step *= fps\r\n",
        "      window_size *= fps\r\n",
        "    res = []\r\n",
        "    n_obs = window_size\r\n",
        "    while n_obs <= self.total_frame:\r\n",
        "      start, end = n_obs-window_size, n_obs\r\n",
        "      obs = self.obs_by_frame_index(start=start, end=end)\r\n",
        "      last_action = self.action_by_frame_index(n_obs-1)\r\n",
        "      action_label = []\r\n",
        "      for t in ts:\r\n",
        "        frame = n_obs + int(fps*t)\r\n",
        "        action_label.append(self.action_by_frame_index(frame-1))\r\n",
        "      res.append({\"observation\":obs, \"obs_frame\":(start, end), \"last_action\":last_action, \"tsec\":ts, \"action_labels\":action_label})\r\n",
        "      n_obs += step\r\n",
        "    return res"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8WLqDrKFu2R"
      },
      "source": [
        "- Produce observations and explanations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNwFJAJxF3-k"
      },
      "source": [
        "def annotation2(video, step=10, window_size=30, ts=[1, 3, 5, 7, 10, 20, 30, 40, 50, 60]):\r\n",
        "  \"\"\"return the list for annotation of the video. One element will be:\r\n",
        "  [file_name,goal,obs_frame_range,obs_time_range,second_last_action,last_action,t,action,explanation,explanation_type] \r\n",
        "  \"\"\"\r\n",
        "  res = []\r\n",
        "  file_name = video.jsonName[\"file_name\"]\r\n",
        "  goal = video.jsonName[\"activity\"]\r\n",
        "  labels_obs = video.label_obs(ts=ts,step=step,window_size=window_size)\r\n",
        "  for obs_info in labels_obs:\r\n",
        "    obs = obs_info[\"observation\"]\r\n",
        "    start, end = obs_info[\"obs_frame\"]\r\n",
        "    obs_frame_range = str(start) + ':' + str(end)\r\n",
        "    obs_time_range = str(np.round(start/video.fps,decimals=2)) + ':' + str(np.round(end/video.fps,decimals=2))\r\n",
        "    second_last_action = None\r\n",
        "    last_action, last_action_index = obs_info[\"last_action\"]\r\n",
        "    if len(obs) > 1:\r\n",
        "      second_last_action = obs[-2][0]\r\n",
        "    for i in range(len(ts)):\r\n",
        "      t = ts[i]\r\n",
        "      action, action_index = obs_info[\"action_labels\"][i]\r\n",
        "      index_diff = action_index - last_action_index\r\n",
        "      explanation = \"\"  # may need to be done manually\r\n",
        "      explanation_type = \"None\"\r\n",
        "      if action is None:\r\n",
        "        explanation = \"None\"\r\n",
        "      elif last_action is None:\r\n",
        "        explanation = \"Because the person just starts with this action \" + action\r\n",
        "      elif index_diff == 0:\r\n",
        "        explanation = \"Because the last action \" + action + \" is not finished\"\r\n",
        "        explanation_type = 0\r\n",
        "      elif index_diff < 4:\r\n",
        "        constituting = {1: \"\", 2: \"second \", 3: \"third \"}\r\n",
        "        explanation = \"Because the person will do the next \" + constituting[index_diff] + \"action \" + action + \" after the current action \" + last_action\r\n",
        "        explanation_type = index_diff\r\n",
        "      else:\r\n",
        "        explanation = \"Because the person will do this action \" + action + \" after the current action \" + last_action\r\n",
        "        explanation_type = 3\r\n",
        "#      if action is not None and (goal in action or index_diff > 3):\r\n",
        "#        explanation += \" and the goal is to make \" + goal\r\n",
        "      res.append([file_name,goal,obs_frame_range,obs_time_range,second_last_action,last_action,t,action,explanation,explanation_type])\r\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf8JUmwKGR5u"
      },
      "source": [
        "- Write into a CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2PKcSZBGVKX"
      },
      "source": [
        "f = open(\"annotation_breakfast2.csv\", 'w')\r\n",
        "f.write(\"file_name,goal,obs_frame_range,obs_time_range,second_last_action,last_action,t,action,explanation,explanation_type\\n\")\r\n",
        "counter = 0\r\n",
        "for fileN in data_files:\r\n",
        "  video = VideoStream2(path,fileN)\r\n",
        "  lines = annotation2(video,step=10)\r\n",
        "  for line in lines:\r\n",
        "    line = ','.join([str(item) for item in line]) + \"\\n\"\r\n",
        "    f.write(line)\r\n",
        "  counter += 1\r\n",
        "  if counter % 100 == 0:\r\n",
        "    print(\"finished file:\",counter)\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkbzF4-83wVN"
      },
      "source": [
        "# Transformer part 1: output without explanation\r\n",
        "reference: [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaGyiCwAg-mn"
      },
      "source": [
        "## Set up input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHQ5QDu9Gg83"
      },
      "source": [
        "1. Reduce the observations into 1 fps and produce the features. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kKDw9TqdhXl",
        "outputId": "c0e3067e-b597-4785-f94a-9788b7c508dc"
      },
      "source": [
        "# action exploration for the Breakfast dataset\r\n",
        "action_set = set()\r\n",
        "for fileN in data_files:\r\n",
        "  video = VideoStream2(path,fileN)\r\n",
        "  for pairs in video.stream:\r\n",
        "    if pairs[0] not in action_set:\r\n",
        "      action_set.add(pairs[0])\r\n",
        "\r\n",
        "print(action_set)\r\n",
        "len(action_set)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'fry_pancake', 'put_fruit2bowl', 'pour_sugar', 'smear_butter', 'peel_fruit', 'pour_juice', 'cut_orange', 'take_topping', 'stir_tea', 'take_glass', 'pour_coffee', 'put_toppingOnTop', 'spoon_powder', 'pour_oil', 'stir_coffee', 'stir_cereals', 'fry_egg', 'stir_milk', 'take_butter', 'take_knife', 'put_pancake2plate', 'pour_water', 'pour_milk', 'stir_fruit', 'take_plate', 'stirfry_egg', 'cut_fruit', 'pour_flour', 'spoon_flour', 'add_teabag', 'stir_dough', 'cut_bun', 'take_squeezer', 'stir_egg', 'add_saltnpepper', 'put_egg2plate', 'pour_egg2pan', 'take_eggs', 'squeeze_orange', 'pour_dough2pan', 'take_bowl', 'spoon_sugar', 'pour_cereals', 'butter_pan', 'take_cup', 'crack_egg', 'put_bunTogether'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6KLAl3-Z3GK"
      },
      "source": [
        "# encode and decode method for one-hot code of actions\r\n",
        "action_lst = []\r\n",
        "action_dict = {}\r\n",
        "index = 0\r\n",
        "for action in action_set:\r\n",
        "  if action not in action_dict:\r\n",
        "    action_lst.append(action)\r\n",
        "    action_dict[action] = index\r\n",
        "    index += 1\r\n",
        "\"\"\"\r\n",
        "def action_encode(action):\r\n",
        "  onehot = np.zeros(len(action_set),dtype=int)\r\n",
        "  if action in action_dict:\r\n",
        "    onehot[action_dict[action]] = 1\r\n",
        "  return onehot\r\n",
        "\r\n",
        "def action_decode(onehot):\r\n",
        "  for index in range(len(onehot)):\r\n",
        "    if onehot[index] == 1:\r\n",
        "      return action_lst[index]\r\n",
        "  return None\r\n",
        "\"\"\"\r\n",
        "def action_encode(action):\r\n",
        "  if action is not None and action in action_dict:\r\n",
        "    return action_dict[action] + 1\r\n",
        "  return 0\r\n",
        "\r\n",
        "def action_decode(token):\r\n",
        "  if token > 0 and token <= len(action_lst):\r\n",
        "    return action_lst[token-1]\r\n",
        "  return None"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EdnXkd8CzRn"
      },
      "source": [
        "q = 30\r\n",
        "fps = 15\r\n",
        "ts=[1, 3, 5, 7, 10, 20, 30, 40, 50, 60]\r\n",
        "step = 10"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSS1_AhJobbR"
      },
      "source": [
        "def build_features(obs):\r\n",
        "  features = []\r\n",
        "  remain = 0\r\n",
        "\r\n",
        "  for action, frame_count in obs:\r\n",
        "    if remain > 0:\r\n",
        "      frame_count += remain - fps\r\n",
        "      if remain < fps/2:\r\n",
        "        features += [action_encode(action)]\r\n",
        "    i = int(frame_count/fps)\r\n",
        "    features += [action_encode(action)] * i\r\n",
        "    remain = frame_count - i * fps\r\n",
        "#    if remain<0 or remain>=fps:\r\n",
        "#      print(obs, action, frame_count, i, remain)\r\n",
        "#      raise Exception()\r\n",
        "    if remain >= fps/2:\r\n",
        "      features += [action_encode(action)]\r\n",
        "\r\n",
        "  return np.array(features)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EUW5gtvAYVt"
      },
      "source": [
        "2. Build the training dataset and cross validation: fold split\r\n",
        "\r\n",
        "- s1: P03 – P15\r\n",
        "- s2: P16 – P28\r\n",
        "- s3: P29 – P41\r\n",
        "- s4: P42 – P54"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-4Lf60sAVYM",
        "outputId": "f4c7790d-b0d3-4a72-94f2-db60907bcdc6"
      },
      "source": [
        "train_obs_features = [[] for _ in range(4)]\r\n",
        "train_tar_action = [[] for _ in range(4)]\r\n",
        "train_exp_type = [[] for _ in range(4)]\r\n",
        "train_exp = [[] for _ in range(4)]\r\n",
        "\r\n",
        "for fileN in data_files:\r\n",
        "  video = VideoStream2(path,fileN)\r\n",
        "  person = int(video.jsonName[\"person\"][1:])\r\n",
        "  labels_obs = video.label_obs(ts=ts,step=step,window_size=q)\r\n",
        "  for obs_info in labels_obs:\r\n",
        "    obs = obs_info[\"observation\"]\r\n",
        "    last_action, last_action_index = obs_info[\"last_action\"]\r\n",
        "\r\n",
        "    target_action = []\r\n",
        "    target_explanation = []\r\n",
        "    target_explanation_type = []\r\n",
        "\r\n",
        "    for i in range(len(ts)):\r\n",
        "      t = ts[i]\r\n",
        "      action, action_index = obs_info[\"action_labels\"][i]\r\n",
        "      index_diff = action_index - last_action_index\r\n",
        "      explanation = \"\"\r\n",
        "      explanation_type = np.zeros(4)\r\n",
        "      if action is None:\r\n",
        "        explanation = \"None\"\r\n",
        "      elif index_diff == 0:\r\n",
        "        explanation = \"Because the last action \" + action + \" is not finished\"\r\n",
        "        explanation_type[0] = 1\r\n",
        "      elif index_diff < 4:\r\n",
        "        constituting = {1: \"\", 2: \"second \", 3: \"third \"}\r\n",
        "        explanation = \"Because the person will do the next \" + constituting[index_diff] + \"action \" + action + \" after the current action \" + last_action\r\n",
        "        explanation_type[index_diff] = 1\r\n",
        "      else:\r\n",
        "        explanation = \"Because the person will do this action \" + action + \" after the current action \" + last_action\r\n",
        "        explanation_type[-1] = 1\r\n",
        "      \r\n",
        "      target_action.append(action_encode(action))\r\n",
        "      target_explanation.append(np.array([explanation]))\r\n",
        "      target_explanation_type.append(explanation_type)\r\n",
        "    \r\n",
        "    if person < 16:\r\n",
        "      split = 0\r\n",
        "    elif person < 29:\r\n",
        "      split = 1\r\n",
        "    elif person < 42:\r\n",
        "      split = 2\r\n",
        "    else:\r\n",
        "      split = 3\r\n",
        "    train_obs_features[split].append(tf.constant(build_features(obs)))\r\n",
        "    train_tar_action[split].append(tf.constant(target_action))\r\n",
        "    train_exp_type[split].append(tf.constant(target_explanation_type))\r\n",
        "    train_exp[split].append(tf.constant(target_explanation))\r\n",
        "\r\n",
        "[len(train_obs_features[s]) for s in range(4)]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2526, 4861, 4443, 6208]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a7gC3sydGAq"
      },
      "source": [
        "save dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU8cifJiJJXj"
      },
      "source": [
        "train_dataset = [[] for _ in range(4)]\r\n",
        "\r\n",
        "for s in range(4):\r\n",
        "  train_dataset[s] = tf.data.Dataset.from_tensor_slices((train_obs_features[s],train_tar_action[s],train_exp_type[s],train_exp[s]))\r\n",
        "  tf.data.experimental.save(train_dataset[s], \"/content/drive/MyDrive/FurtherStudy/Project2020/breakfast_data_s\"+str(s+1))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMtIOpcoKMhc",
        "outputId": "e558b3fd-8436-4d72-c1f7-6f2cf5c2a48e"
      },
      "source": [
        "next(iter(train_dataset[0]))[:2]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 30), dtype=int64, numpy=\n",
              " array([[43, 43, 43, ..., 43, 43, 43],\n",
              "        [26, 26, 26, ..., 26, 26, 26],\n",
              "        [43, 43, 43, ..., 41, 10, 10],\n",
              "        ...,\n",
              "        [ 5,  5,  5, ...,  5,  5,  5],\n",
              "        [ 5,  5,  5, ...,  5,  5,  5],\n",
              "        [ 5,  5,  5, ...,  5,  5,  5]])>,\n",
              " <tf.Tensor: shape=(64, 10), dtype=int32, numpy=\n",
              " array([[43, 43, 43, 43, 43, 28, 10, 10, 10, 10],\n",
              "        [26, 26, 26, 26, 39, 39,  5,  5,  5,  5],\n",
              "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
              "        [23, 23, 23, 23, 23, 31,  0,  0,  0,  0],\n",
              "        [ 8,  8,  8, 29, 29,  0,  0,  0,  0,  0],\n",
              "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
              "        [ 5,  5,  5,  5,  5,  5,  5, 33,  0,  0],\n",
              "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
              "        [32, 32, 32, 32, 32,  0,  0,  0,  0,  0],\n",
              "        [ 8,  8,  8,  8,  8,  8,  8, 29, 29,  0],\n",
              "        [42,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 8,  8,  8,  8,  8, 34, 29, 29,  0,  0],\n",
              "        [32, 32, 32, 32, 32, 32, 24, 24, 32, 32],\n",
              "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
              "        [ 7,  7, 24, 24, 24, 33, 33, 13, 13, 13],\n",
              "        [15, 15,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [32, 32, 32, 32, 32, 32, 32, 32,  0,  0],\n",
              "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
              "        [43, 43, 43, 41, 41, 10, 10, 10, 10, 10],\n",
              "        [10, 10, 10, 10, 10, 10, 10, 47,  0,  0],\n",
              "        [15, 15,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [31, 31, 31,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [35, 35, 26, 26, 39, 39,  5,  5,  5,  5],\n",
              "        [26, 26, 26, 26, 26, 26, 26, 26, 46, 46],\n",
              "        [42, 42, 42, 42,  0,  0,  0,  0,  0,  0],\n",
              "        [15, 15, 15, 15, 15, 15, 15, 15,  0,  0],\n",
              "        [13, 15, 15, 15, 15,  0,  0,  0,  0,  0],\n",
              "        [ 9,  9,  9,  9,  9,  9,  9,  9, 42,  0],\n",
              "        [24, 24, 24, 24, 24, 32, 32, 32, 32, 33],\n",
              "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
              "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
              "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
              "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
              "        [10, 10, 10, 10, 10, 10, 10, 47, 10, 10],\n",
              "        [21, 21, 31, 31, 31, 26, 26, 26, 26, 26],\n",
              "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
              "        [47, 47, 47, 47, 47, 10, 10, 43, 43, 43],\n",
              "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
              "        [ 8,  8,  8,  8, 29, 29,  0,  0,  0,  0],\n",
              "        [10, 10, 10, 10, 10, 47,  0,  0,  0,  0],\n",
              "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
              "        [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
              "        [43, 43, 43, 28, 28, 10, 10, 10, 10, 47],\n",
              "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
              "        [ 8,  8,  8,  8,  8,  8, 29, 29,  0,  0],\n",
              "        [ 7,  7,  7,  7,  7, 24, 24, 13, 13, 13],\n",
              "        [24, 24, 32, 32, 32, 32, 32, 32, 32, 32],\n",
              "        [ 5,  5,  5,  5,  5, 33,  0,  0,  0,  0],\n",
              "        [13, 13, 13, 33, 33, 13, 13, 13, 13, 13],\n",
              "        [21, 21, 31, 31, 31, 26, 26, 26, 26, 26],\n",
              "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
              "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
              "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\n",
              "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
              "        [43, 43, 43, 43, 43, 43, 10, 10, 10, 10],\n",
              "        [24, 24, 24, 24, 24, 12, 36, 32, 32, 32],\n",
              "        [24, 24, 15, 15, 15, 15,  0,  0,  0,  0],\n",
              "        [31, 31, 31, 31, 26, 26, 26, 26, 26, 26],\n",
              "        [26, 26, 26, 26, 26, 39,  5,  5,  5,  5],\n",
              "        [24, 24, 24, 24, 24, 32, 32, 32, 32, 33],\n",
              "        [26, 26, 26, 26, 26, 26, 26, 26, 39, 39],\n",
              "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
              "        [ 5,  5,  5,  5,  5,  5, 33, 33,  0,  0],\n",
              "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5]], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20YCUa7xdL03"
      },
      "source": [
        "load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpvdqzekVlhQ"
      },
      "source": [
        "# load from saved files\r\n",
        "def load_dataset(path):\r\n",
        "  return tf.data.experimental.load(\r\n",
        "    path,\r\n",
        "    (tf.TensorSpec(shape=(30, ), dtype=tf.int64, name=None),\r\n",
        "     tf.TensorSpec(shape=(10, ), dtype=tf.int32, name=None),\r\n",
        "     tf.TensorSpec(shape=(10, 4), dtype=tf.float64, name=None),\r\n",
        "     tf.TensorSpec(shape=(10, 1), dtype=tf.string, name=None)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II8yEg-rcoJD"
      },
      "source": [
        "train_dataset = [[] for _ in range(4)]\r\n",
        "dataset_path = \"/content/drive/MyDrive/FurtherStudy/Project2020/breakfast_data_s\"\r\n",
        "for s in range(4):\r\n",
        "  train_dataset[s] = load_dataset(dataset_path+str(s+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3OizkL2dP1H"
      },
      "source": [
        "shuffle and other things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa3LVjzn-uyO"
      },
      "source": [
        "BUFFER_SIZE = 20000\r\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhbxKvyGdodC"
      },
      "source": [
        "\"\"\"# split train-val: size = (17k,1k)\r\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\r\n",
        "val_dataset = train_dataset.take(1000) \r\n",
        "train_dataset = train_dataset.skip(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hShLByZGaBl2"
      },
      "source": [
        "# speedup, shuffle and batch\r\n",
        "\"\"\"\r\n",
        "train_dataset = train_dataset.cache()\r\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=True)\r\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\r\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\r\n",
        "\r\n",
        "val_dataset = val_dataset.padded_batch(BATCH_SIZE)\r\n",
        "\"\"\"\r\n",
        "for s in range(4):\r\n",
        "  train_dataset[s] = train_dataset[s].cache()\r\n",
        "  train_dataset[s] = train_dataset[s].shuffle(BUFFER_SIZE,reshuffle_each_iteration=True)\r\n",
        "  train_dataset[s] = train_dataset[s].padded_batch(BATCH_SIZE)\r\n",
        "  train_dataset[s] = train_dataset[s].prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUoJ48ofeRMq"
      },
      "source": [
        "obs_feature, tar_action, expl_type, expl = next(iter(train_dataset[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKRtMvN0mRxK"
      },
      "source": [
        "## Transformer Model\r\n",
        "\r\n",
        "https://keras.io/examples/nlp/text_classification_with_transformer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OKGzm5MnhFE"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ2W_et8mW24"
      },
      "source": [
        "# Position-wise Feed-Forward Networks\r\n",
        "def pw_ffn(d_model, dff):\r\n",
        "  \"\"\"\r\n",
        "  output dim: d_model\r\n",
        "  inner layer dim: dff\r\n",
        "  \"\"\"\r\n",
        "  return tf.keras.Sequential([\r\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\r\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\r\n",
        "  ])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WupxTOuCoZIs"
      },
      "source": [
        "# attention\r\n",
        "# tf.keras.layers.MultiHeadAttention(num_heads=?, key_dim=d_model)\r\n",
        "# norm layer\r\n",
        "# tf.keras.layers.LayerNormalization(epsilon=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQrHFAFTnpe_"
      },
      "source": [
        "class TransformerLayer(tf.keras.layers.Layer):\r\n",
        "  \"\"\"\r\n",
        "  mha -> normal -> ffn -> normal\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, d_model, num_heads, dff, dropout):\r\n",
        "    super(TransformerLayer, self).__init__()\r\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout)\r\n",
        "    self.ffn = pw_ffn(d_model, dff)\r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)   \r\n",
        "    \r\n",
        "  def call(self, x):\r\n",
        "    attn = self.mha(x, x)  # (batch_size, timesteps, d_model)\r\n",
        "    out1 = self.layernorm1(attn + x)\r\n",
        "    ffn_output = self.ffn(out1)  \r\n",
        "    out2 = self.layernorm2(ffn_output + out1)  # (batch_size, timesteps, d_model)\r\n",
        "    return out2"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiDiQn2S3HY2"
      },
      "source": [
        "class Transformer(tf.keras.Model):\r\n",
        "  \"\"\"\r\n",
        "  d_model = dim_action_embed + dim_time_embed\r\n",
        "\r\n",
        "  Arch:\r\n",
        "  Concat(action_embedding, time_embedding)  (batch_size, timesteps, d_model)\r\n",
        "  -> N * Transformer layer                  (same shape)\r\n",
        "  -> Average on timestep                    (batch_size, d_model)\r\n",
        "  -> Softmax FC                             (batch_size, action_size)\r\n",
        "\r\n",
        "  Output: batch_size probability vectors\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, num_layers, action_embed_dim, time_embed_dim, num_heads, dff, time_size, action_size, dropout):\r\n",
        "    super(Transformer, self).__init__()\r\n",
        "    self.d_model = action_embed_dim + time_embed_dim\r\n",
        "    self.num_layers = num_layers\r\n",
        "    \r\n",
        "    self.action_embedding = tf.keras.layers.Embedding(action_size, action_embed_dim)\r\n",
        "    self.time_embedding = tf.keras.layers.Embedding(time_size, time_embed_dim)\r\n",
        "    self.transformer_layers = [TransformerLayer(d_model, num_heads, dff, dropout)  for _ in range(num_layers)]\r\n",
        "    self.average_on_timestep = tf.keras.layers.GlobalAveragePooling1D()\r\n",
        "    self.final_layer = tf.keras.layers.Dense(action_size, activation=\"softmax\")\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    x, t = x[:, :-1], x[:, -1]\r\n",
        "    # (x.shape, t.shape) = ((batch_size, timesteps), (batch_size, ))\r\n",
        "    ut = self.time_embedding(t)  \r\n",
        "    ut = tf.repeat(ut[:, np.newaxis, :], x.shape[1], axis=1)\r\n",
        "\r\n",
        "    x = self.action_embedding(x)  \r\n",
        "    x = tf.concat([x, ut], axis=2)     # (batch_size, timesteps, d_model)\r\n",
        "    \r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x = self.transformer_layers[i](x)\r\n",
        "    \r\n",
        "    x = self.average_on_timestep(x)\r\n",
        "    action_prob = self.final_layer(x)\r\n",
        "    return action_prob"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsh7wcwk1RqJ"
      },
      "source": [
        "# hyperparameters\r\n",
        "num_layers = 4\r\n",
        "d_model = 128\r\n",
        "dff = 512\r\n",
        "num_heads = 8\r\n",
        "time_embed_dim = 4\r\n",
        "action_embed_dim = d_model - time_embed_dim\r\n",
        "time_size = 61\r\n",
        "action_size = 48\r\n",
        "dropout = 0"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deuo0GehQX6H"
      },
      "source": [
        "transformer = Transformer(num_layers, action_embed_dim, time_embed_dim, num_heads, dff, time_size, action_size, dropout)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3RlaDA2QnpE",
        "outputId": "c3db6053-a427-4a64-fc7a-21c612cb34dc"
      },
      "source": [
        "# try it out\r\n",
        "feature_vec, action_gts = next(iter(train_dataset[0]))[:2]\r\n",
        "t = tf.constant([1] * BATCH_SIZE, dtype=tf.int64)\r\n",
        "print(feature_vec.shape)\r\n",
        "print(t.shape)\r\n",
        "inputs = tf.concat([feature_vec, t[:, np.newaxis]], axis=1)\r\n",
        "print(inputs.shape)\r\n",
        "transformer(inputs)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 30)\n",
            "(64,)\n",
            "(64, 31)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 48), dtype=float32, numpy=\n",
              "array([[0.0126037 , 0.05033874, 0.02474671, ..., 0.05431449, 0.01325358,\n",
              "        0.05294066],\n",
              "       [0.00471926, 0.00488957, 0.01696076, ..., 0.03369428, 0.01474323,\n",
              "        0.01867529],\n",
              "       [0.00800071, 0.03373576, 0.00163424, ..., 0.00884827, 0.01512288,\n",
              "        0.0033734 ],\n",
              "       ...,\n",
              "       [0.00186551, 0.00411144, 0.02881517, ..., 0.01832406, 0.00557621,\n",
              "        0.01114837],\n",
              "       [0.00167478, 0.00840278, 0.00268359, ..., 0.01019125, 0.00353074,\n",
              "        0.00483252],\n",
              "       [0.00150605, 0.0076069 , 0.00227459, ..., 0.0091827 , 0.00305254,\n",
              "        0.00406044]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx50yW-DZetn"
      },
      "source": [
        "## Training & evaluation by 4-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB4PU2HtZRbt"
      },
      "source": [
        "learning_rate = 1e-3\r\n",
        "EPOCHS = 10\r\n",
        "#optimizer = tf.optimizers.Adam(learning_rate, decay=learning_rate/EPOCHS)\r\n",
        "optimizer = tf.optimizers.Adam(learning_rate)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuRK2qvLDn9x"
      },
      "source": [
        "def fitting_generator(dataset):\r\n",
        "  \"\"\"\r\n",
        "  a generator yielding the inputs and labels; dataset: tf.data.Dataset\r\n",
        "  \"\"\"\r\n",
        "  for x in iter(dataset):\r\n",
        "    inputs, action_gts = x[0], x[1]\r\n",
        "    batch_size = x[1].shape[0]\r\n",
        "    t_lst, targets = [], []\r\n",
        "    for bs in range(batch_size):\r\n",
        "      imax = 1\r\n",
        "      while imax < 10 and action_gts[bs, imax].numpy() > 0:\r\n",
        "        imax += 1\r\n",
        "      i = np.random.randint(imax)\r\n",
        "      targets.append(action_gts[bs, i].numpy())\r\n",
        "      t_lst.append(ts[i])\r\n",
        "    inputs = tf.concat([inputs, tf.constant(t_lst, dtype=tf.int64)[:, np.newaxis]], axis=1) \r\n",
        "    targets = tf.constant(targets)\r\n",
        "    yield inputs, targets\r\n",
        "\r\n",
        "def fitting_validation_dataset(dataset):\r\n",
        "  input_lst, target_lst = [], []\r\n",
        "  for x in iter(dataset):\r\n",
        "    inputs, action_gts = x[0], x[1]\r\n",
        "    batch_size = inputs.shape[0]\r\n",
        "    for bs in range(batch_size):\r\n",
        "      imax = 1\r\n",
        "      while imax < 10 and action_gts[bs, imax].numpy() > 0:\r\n",
        "        imax += 1\r\n",
        "      i = np.random.randint(imax)\r\n",
        "      input = np.append(inputs[bs, :].numpy(), ts[i])  # [x, t]\r\n",
        "      target = np.zeros(action_size, dtype=int)\r\n",
        "      target[action_gts[bs, i].numpy()] = 1  # one-hot code of the action\r\n",
        "      input_lst.append(input)\r\n",
        "      target_lst.append(target)\r\n",
        "  res_set = tf.data.Dataset.from_tensor_slices((input_lst, target_lst))\r\n",
        "  return res_set.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YPuBget2wH4",
        "outputId": "b6363958-6174-4519-dd7c-d39fc8eea186"
      },
      "source": [
        "new_train_ds = [fitting_validation_dataset(train_dataset[s]) for s in range(4)]\r\n",
        "next(iter(new_train_ds[0]))\r\n",
        "# np.sum(next(iter(new_train_ds[0]))[1]) == 64"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 31), dtype=int64, numpy=\n",
              " array([[46, 46, 46, ..., 46, 46,  7],\n",
              "        [13, 13, 13, ..., 23, 23,  7],\n",
              "        [25, 25, 25, ..., 39, 39,  3],\n",
              "        ...,\n",
              "        [12, 12, 12, ..., 12, 12,  3],\n",
              "        [ 5,  5,  5, ..., 27, 27, 50],\n",
              "        [17, 17, 36, ..., 36, 36, 20]])>,\n",
              " <tf.Tensor: shape=(64, 48), dtype=int64, numpy=\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iV8TAwimiNZ",
        "outputId": "63bb1223-cff8-42e9-ac66-538ad4966f7d"
      },
      "source": [
        "history = [None for _ in range(4)]\r\n",
        "for s in range(4):\r\n",
        "  print(\"------------------------------------------------------\")\r\n",
        "  print(\"Take s\"+str(s+1)+\" dataset as the testing set and concatenate the other 3 for training.\\n\")\r\n",
        "  test_set = new_train_ds[s]\r\n",
        "  tr_is = list(set(range(4)) - {s})\r\n",
        "  training_set = new_train_ds[tr_is[0]].concatenate(new_train_ds[tr_is[1]]).concatenate(new_train_ds[tr_is[2]])\r\n",
        "  transformer = Transformer(num_layers, action_embed_dim, time_embed_dim, num_heads, dff, time_size, action_size, dropout)\r\n",
        "  optimizer = tf.optimizers.Adam(learning_rate)\r\n",
        "  transformer.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\r\n",
        "  history[s] = transformer.fit(x=training_set, validation_data=test_set, epochs=EPOCHS)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------\n",
            "Take s1 dataset as the testing set and concatenate the other 3 for training.\n",
            "\n",
            "Epoch 1/10\n",
            "243/243 [==============================] - 275s 1s/step - loss: 1.2171 - accuracy: 0.6348 - val_loss: 0.9763 - val_accuracy: 0.7074\n",
            "Epoch 2/10\n",
            "243/243 [==============================] - 272s 1s/step - loss: 0.8509 - accuracy: 0.7248 - val_loss: 0.9355 - val_accuracy: 0.7098\n",
            "Epoch 3/10\n",
            "243/243 [==============================] - 272s 1s/step - loss: 0.7939 - accuracy: 0.7325 - val_loss: 0.9363 - val_accuracy: 0.7181\n",
            "Epoch 4/10\n",
            "243/243 [==============================] - 271s 1s/step - loss: 0.7735 - accuracy: 0.7340 - val_loss: 0.9788 - val_accuracy: 0.7118\n",
            "Epoch 5/10\n",
            "243/243 [==============================] - 270s 1s/step - loss: 0.7570 - accuracy: 0.7380 - val_loss: 0.9626 - val_accuracy: 0.7201\n",
            "Epoch 6/10\n",
            "243/243 [==============================] - 271s 1s/step - loss: 0.7519 - accuracy: 0.7449 - val_loss: 0.9282 - val_accuracy: 0.7225\n",
            "Epoch 7/10\n",
            "243/243 [==============================] - 272s 1s/step - loss: 0.7402 - accuracy: 0.7453 - val_loss: 0.9236 - val_accuracy: 0.7173\n",
            "Epoch 8/10\n",
            "243/243 [==============================] - 272s 1s/step - loss: 0.7695 - accuracy: 0.7386 - val_loss: 0.9980 - val_accuracy: 0.7019\n",
            "Epoch 9/10\n",
            "243/243 [==============================] - 270s 1s/step - loss: 0.8092 - accuracy: 0.7270 - val_loss: 0.9110 - val_accuracy: 0.7264\n",
            "Epoch 10/10\n",
            "243/243 [==============================] - 270s 1s/step - loss: 0.7458 - accuracy: 0.7467 - val_loss: 0.9175 - val_accuracy: 0.7217\n",
            "------------------------------------------------------\n",
            "Take s2 dataset as the testing set and concatenate the other 3 for training.\n",
            "\n",
            "Epoch 1/10\n",
            "207/207 [==============================] - 250s 1s/step - loss: 1.3365 - accuracy: 0.6190 - val_loss: 1.0273 - val_accuracy: 0.6990\n",
            "Epoch 2/10\n",
            "207/207 [==============================] - 245s 1s/step - loss: 0.8880 - accuracy: 0.7165 - val_loss: 1.0053 - val_accuracy: 0.6900\n",
            "Epoch 3/10\n",
            "207/207 [==============================] - 246s 1s/step - loss: 0.8337 - accuracy: 0.7308 - val_loss: 0.9716 - val_accuracy: 0.7029\n",
            "Epoch 4/10\n",
            "207/207 [==============================] - 245s 1s/step - loss: 0.8065 - accuracy: 0.7345 - val_loss: 0.9168 - val_accuracy: 0.7149\n",
            "Epoch 5/10\n",
            "207/207 [==============================] - 246s 1s/step - loss: 0.7905 - accuracy: 0.7389 - val_loss: 0.9731 - val_accuracy: 0.7060\n",
            "Epoch 6/10\n",
            "207/207 [==============================] - 244s 1s/step - loss: 0.7806 - accuracy: 0.7435 - val_loss: 0.9460 - val_accuracy: 0.7182\n",
            "Epoch 7/10\n",
            "207/207 [==============================] - 243s 1s/step - loss: 0.7831 - accuracy: 0.7385 - val_loss: 0.9539 - val_accuracy: 0.7087\n",
            "Epoch 8/10\n",
            "207/207 [==============================] - 244s 1s/step - loss: 0.7689 - accuracy: 0.7481 - val_loss: 0.9502 - val_accuracy: 0.7040\n",
            "Epoch 9/10\n",
            "207/207 [==============================] - 244s 1s/step - loss: 0.7679 - accuracy: 0.7452 - val_loss: 0.9865 - val_accuracy: 0.7046\n",
            "Epoch 10/10\n",
            "207/207 [==============================] - 244s 1s/step - loss: 0.7743 - accuracy: 0.7437 - val_loss: 0.9520 - val_accuracy: 0.7153\n",
            "------------------------------------------------------\n",
            "Take s3 dataset as the testing set and concatenate the other 3 for training.\n",
            "\n",
            "Epoch 1/10\n",
            "213/213 [==============================] - 255s 1s/step - loss: 1.2714 - accuracy: 0.6272 - val_loss: 0.9890 - val_accuracy: 0.6660\n",
            "Epoch 2/10\n",
            "213/213 [==============================] - 250s 1s/step - loss: 0.8737 - accuracy: 0.7117 - val_loss: 0.9104 - val_accuracy: 0.7148\n",
            "Epoch 3/10\n",
            "213/213 [==============================] - 250s 1s/step - loss: 0.8154 - accuracy: 0.7278 - val_loss: 0.8953 - val_accuracy: 0.7133\n",
            "Epoch 4/10\n",
            "213/213 [==============================] - 250s 1s/step - loss: 0.7873 - accuracy: 0.7355 - val_loss: 0.8696 - val_accuracy: 0.7200\n",
            "Epoch 5/10\n",
            "213/213 [==============================] - 250s 1s/step - loss: 0.7734 - accuracy: 0.7416 - val_loss: 0.8812 - val_accuracy: 0.7189\n",
            "Epoch 6/10\n",
            "213/213 [==============================] - 249s 1s/step - loss: 0.7665 - accuracy: 0.7403 - val_loss: 0.8780 - val_accuracy: 0.7241\n",
            "Epoch 7/10\n",
            "213/213 [==============================] - 249s 1s/step - loss: 0.7588 - accuracy: 0.7404 - val_loss: 0.8684 - val_accuracy: 0.7315\n",
            "Epoch 8/10\n",
            "213/213 [==============================] - 249s 1s/step - loss: 0.7528 - accuracy: 0.7465 - val_loss: 0.9009 - val_accuracy: 0.7214\n",
            "Epoch 9/10\n",
            "213/213 [==============================] - 249s 1s/step - loss: 0.7690 - accuracy: 0.7422 - val_loss: 0.8936 - val_accuracy: 0.7169\n",
            "Epoch 10/10\n",
            "213/213 [==============================] - 249s 1s/step - loss: 0.8136 - accuracy: 0.7300 - val_loss: 0.8861 - val_accuracy: 0.7241\n",
            "------------------------------------------------------\n",
            "Take s4 dataset as the testing set and concatenate the other 3 for training.\n",
            "\n",
            "Epoch 1/10\n",
            "186/186 [==============================] - 242s 1s/step - loss: 1.3058 - accuracy: 0.6214 - val_loss: 1.1287 - val_accuracy: 0.6525\n",
            "Epoch 2/10\n",
            "186/186 [==============================] - 237s 1s/step - loss: 0.8320 - accuracy: 0.7293 - val_loss: 1.0815 - val_accuracy: 0.6759\n",
            "Epoch 3/10\n",
            "186/186 [==============================] - 237s 1s/step - loss: 0.7887 - accuracy: 0.7401 - val_loss: 1.0593 - val_accuracy: 0.6815\n",
            "Epoch 4/10\n",
            "186/186 [==============================] - 236s 1s/step - loss: 0.7672 - accuracy: 0.7433 - val_loss: 1.0295 - val_accuracy: 0.6741\n",
            "Epoch 5/10\n",
            "186/186 [==============================] - 236s 1s/step - loss: 0.7467 - accuracy: 0.7498 - val_loss: 1.0186 - val_accuracy: 0.6819\n",
            "Epoch 6/10\n",
            "186/186 [==============================] - 237s 1s/step - loss: 0.7402 - accuracy: 0.7495 - val_loss: 1.0093 - val_accuracy: 0.6922\n",
            "Epoch 7/10\n",
            "186/186 [==============================] - 236s 1s/step - loss: 0.7280 - accuracy: 0.7563 - val_loss: 1.0342 - val_accuracy: 0.6869\n",
            "Epoch 8/10\n",
            "186/186 [==============================] - 237s 1s/step - loss: 0.7301 - accuracy: 0.7564 - val_loss: 1.0370 - val_accuracy: 0.6794\n",
            "Epoch 9/10\n",
            "186/186 [==============================] - 237s 1s/step - loss: 0.7400 - accuracy: 0.7518 - val_loss: 1.0325 - val_accuracy: 0.6778\n",
            "Epoch 10/10\n",
            "186/186 [==============================] - 237s 1s/step - loss: 0.7231 - accuracy: 0.7524 - val_loss: 1.0314 - val_accuracy: 0.6770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6eer8S37x3x"
      },
      "source": [
        "def predict(transformer, feature_vec, t):\r\n",
        "  \"\"\" prediction of transformer\r\n",
        "  transformer: Transformer; a transformer model.\r\n",
        "  feature_vec: int numpy.array; a feature vector with size (timesteps, ).\r\n",
        "  t: int; a scalar in range(1, 61).\r\n",
        "  return: action string, probability vector, action code in range(48).\r\n",
        "  \"\"\"\r\n",
        "  x = tf.constant(np.append(feature_vec, t))[np.newaxis, :]\r\n",
        "  prob = transformer.predict(x)[0]\r\n",
        "  code = np.argmax(prob)\r\n",
        "  action = action_decode(code)\r\n",
        "  return action, prob, code"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vmMax2XnjAc",
        "outputId": "b906939a-add6-4121-f5cc-a6e508881056"
      },
      "source": [
        "# next(fitting_generator(train_dataset[0]))[:2]\r\n",
        "# new_ds = fitting_validation_dataset(train_dataset[0])\r\n",
        "x1, y1 = next(iter(new_train_ds[0]))\r\n",
        "transformer.predict(x1[0:1, :])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.0744157e-04, 1.5675380e-03, 7.4660376e-05, 1.3063441e-04,\n",
              "        4.9197860e-04, 2.0064093e-04, 2.0653177e-04, 1.1066241e-04,\n",
              "        1.8729082e-04, 7.5888273e-04, 8.8978151e-04, 1.6779260e-04,\n",
              "        1.0581568e-03, 1.0318370e-03, 1.6070422e-03, 5.2686245e-04,\n",
              "        2.1011113e-04, 1.6056523e-02, 1.2942561e-04, 1.6121882e-04,\n",
              "        7.6181954e-04, 2.2817160e-04, 5.4243568e-04, 9.9311303e-03,\n",
              "        8.1982907e-05, 7.4083931e-03, 7.9221539e-03, 8.4808416e-05,\n",
              "        2.9455023e-04, 1.7101312e-01, 5.4530782e-04, 2.6771026e-02,\n",
              "        7.9283770e-04, 2.3331727e-04, 1.5213987e-01, 8.6652793e-02,\n",
              "        7.9626724e-04, 3.7751701e-03, 9.2496208e-05, 1.1192705e-03,\n",
              "        1.6696424e-03, 3.2191243e-04, 1.0995590e-03, 4.4380233e-04,\n",
              "        5.6145426e-02, 1.5494972e-04, 4.4264314e-01, 5.5960804e-04]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaDymVSpB8Ml",
        "outputId": "91e5ac02-6826-400d-e894-45090a25b2c0"
      },
      "source": [
        "x1[0:1, :]"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 31), dtype=int64, numpy=\n",
              "array([[46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46,\n",
              "        46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46,  7]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWZZHmp8CWAc",
        "outputId": "5dc22095-7349-4cd6-ea67-51f3300b851e"
      },
      "source": [
        "predict(transformer, x1[0, :-1].numpy(), x1[0, -1])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('crack_egg',\n",
              " array([2.0744157e-04, 1.5675380e-03, 7.4660376e-05, 1.3063441e-04,\n",
              "        4.9197860e-04, 2.0064093e-04, 2.0653177e-04, 1.1066241e-04,\n",
              "        1.8729082e-04, 7.5888273e-04, 8.8978151e-04, 1.6779260e-04,\n",
              "        1.0581568e-03, 1.0318370e-03, 1.6070422e-03, 5.2686245e-04,\n",
              "        2.1011113e-04, 1.6056523e-02, 1.2942561e-04, 1.6121882e-04,\n",
              "        7.6181954e-04, 2.2817160e-04, 5.4243568e-04, 9.9311303e-03,\n",
              "        8.1982907e-05, 7.4083931e-03, 7.9221539e-03, 8.4808416e-05,\n",
              "        2.9455023e-04, 1.7101312e-01, 5.4530782e-04, 2.6771026e-02,\n",
              "        7.9283770e-04, 2.3331727e-04, 1.5213987e-01, 8.6652793e-02,\n",
              "        7.9626724e-04, 3.7751701e-03, 9.2496208e-05, 1.1192705e-03,\n",
              "        1.6696424e-03, 3.2191243e-04, 1.0995590e-03, 4.4380233e-04,\n",
              "        5.6145426e-02, 1.5494972e-04, 4.4264314e-01, 5.5960804e-04],\n",
              "       dtype=float32),\n",
              " 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q5FZkZErv4D"
      },
      "source": [
        "# Previous attempts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r7csu3C20Vh"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\r\n",
        "  \"\"\"\r\n",
        "  mha -> normal -> ffn -> normal\r\n",
        "  call: output: the last dim is d_model\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, d_model, num_heads, dff, dropout):\r\n",
        "    super(EncoderLayer, self).__init__()\r\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout)\r\n",
        "    self.ffn = pw_ffn(d_model, dff)\r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    \r\n",
        "  def call(self, x):\r\n",
        "    attn_output = self.mha(x, x)  # (batch_size, input_seq_len, d_model)\r\n",
        "    out1 = self.layernorm1(x + attn_output)  \r\n",
        "    ffn_output = self.ffn(out1)  \r\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\r\n",
        "    return out2"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgc0Ki9P7LSL"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\r\n",
        "  \"\"\"\r\n",
        "  mha -> normal -> mha(encoder output) -> normal -> ffn -> normal\r\n",
        "  call: output: the last dim is d_model\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, d_model, input_embed_dim, num_heads, dff, dropout):\r\n",
        "    super(DecoderLayer, self).__init__()\r\n",
        "    self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout)\r\n",
        "    self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout)\r\n",
        "    self.ffn = pw_ffn(d_model, dff)\r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)    \r\n",
        "    \r\n",
        "  def call(self, x, enc_output):\r\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\r\n",
        "    attn1 = self.mha1(x, x)  # (batch_size, target_seq_len, d_model)\r\n",
        "    out1 = self.layernorm1(attn1 + x)\r\n",
        "    attn2 = self.mha2(out1, enc_output)  # make a query on the encoder output values\r\n",
        "    out2 = self.layernorm2(attn2 + out1)  \r\n",
        "    ffn_output = self.ffn(out2)  \r\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\r\n",
        "    return out3"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOyNSgzvKwSn"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, action_size, dropout):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.d_model = d_model\r\n",
        "    self.num_layers = num_layers\r\n",
        "\r\n",
        "    self.embedding = tf.keras.layers.Embedding(action_size, d_model)\r\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)]\r\n",
        "        \r\n",
        "  def call(self, x):\r\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\r\n",
        "    # x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "    \r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x = self.enc_layers[i](x)\r\n",
        "    \r\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8meUJyRLf2Y"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, num_layers, d_model, time_embed_dim, num_heads, dff, time_upper_bound, action_size, dropout):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "    self.d_model = d_model\r\n",
        "    self.num_layers = num_layers\r\n",
        "    \r\n",
        "    self.embedding = tf.keras.layers.Embedding(action_size, d_model)\r\n",
        "    self.time_embedding = tf.keras.layers.Embedding(time_upper_bound, time_embed_dim)\r\n",
        "    self.dec_layers = [DecoderLayer(d_model, d_model+time_embed_dim, num_heads, dff, dropout)] + [DecoderLayer(d_model, d_model, num_heads, dff, dropout)  for _ in range(num_layers-1)]\r\n",
        "    \r\n",
        "  def call(self, x, t, enc_output):\r\n",
        "    print(x, t)\r\n",
        "    ut = self.time_embedding(tf.constant([t]))  \r\n",
        "    ut = tf.repeat(ut[np.newaxis, :, :], x.shape[0], axis=0)\r\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\r\n",
        "    #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "    x = tf.concat([x, ut], axis=2)     # (batch_size, target_seq_len, d_model+time_embed_dim)\r\n",
        "    \r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x = self.dec_layers[i](x, enc_output)\r\n",
        "    \r\n",
        "    return x  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ene-BNXqLgRz"
      },
      "source": [
        "class Transformer(tf.keras.Model):\r\n",
        "  def __init__(self, num_layers, d_model, time_embed_dim, num_heads, dff, time_upper_bound, action_size, dropout=0):\r\n",
        "    super(Transformer, self).__init__()\r\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, action_size, dropout)\r\n",
        "    self.decoder = Decoder(num_layers, d_model, time_embed_dim, num_heads, dff, time_upper_bound, action_size, dropout)\r\n",
        "    self.final_layer = tf.keras.layers.Dense(action_size, activation=\"softmax\")\r\n",
        "    \r\n",
        "  def call(self, feature_vec, action_gt, t):\r\n",
        "    enc_output = self.encoder(feature_vec)  # (batch_size, inp_seq_len, d_model)\r\n",
        "    dec_output = self.decoder(action_gt, t, enc_output)  # (batch_size, tar_seq_len, d_model)\r\n",
        "    action_prob = self.final_layer(dec_output)  # (batch_size, tar_seq_len, action_size)\r\n",
        "    \r\n",
        "    return action_prob"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4wJog_nCb4k"
      },
      "source": [
        "# hyperparameters\r\n",
        "num_layers = 4\r\n",
        "d_model = 128\r\n",
        "dff = 512\r\n",
        "num_heads = 8\r\n",
        "time_embed_dim = 4\r\n",
        "time_upper_bound = 61\r\n",
        "action_size = 48\r\n",
        "dropout = 0"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1BN9NYVC8bQ"
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, time_embed_dim, num_heads, dff, time_upper_bound, action_size, dropout)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2kizhBhD5gt"
      },
      "source": [
        "draft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlWinepfDrXS",
        "outputId": "f853e3ee-9d1f-484c-e7d7-a6c4c69f6a05"
      },
      "source": [
        "feature_vec, action_gts = next(iter(train_dataset[0]))[:2]\r\n",
        "i = np.random.randint(10)\r\n",
        "action_gt, t = action_gts[:, i:i+1], ts[i]\r\n",
        "transformer(feature_vec, t)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 30, 4)\n",
            "(64, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 48), dtype=float32, numpy=\n",
              "array([[0.02333895, 0.15978588, 0.01308024, ..., 0.01351934, 0.02541033,\n",
              "        0.0062274 ],\n",
              "       [0.02333895, 0.15978588, 0.01308024, ..., 0.01351934, 0.02541033,\n",
              "        0.0062274 ],\n",
              "       [0.02457357, 0.10944364, 0.01247965, ..., 0.01521281, 0.04351594,\n",
              "        0.00712113],\n",
              "       ...,\n",
              "       [0.02803414, 0.07099845, 0.0010472 , ..., 0.03021072, 0.02950123,\n",
              "        0.07050299],\n",
              "       [0.02333895, 0.15978588, 0.01308024, ..., 0.01351934, 0.02541033,\n",
              "        0.0062274 ],\n",
              "       [0.00479883, 0.0237511 , 0.00969952, ..., 0.00572318, 0.06821384,\n",
              "        0.00706416]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biyJQfOTyjug",
        "outputId": "3a59435e-5a36-4ff2-cfc7-9f3895ceefe7"
      },
      "source": [
        "test = tf.keras.layers.Embedding(5,4)\r\n",
        "a = test(tf.constant([1]))\r\n",
        "print(a)\r\n",
        "b=tf.repeat(a[np.newaxis,:,:], 3,axis=0)\r\n",
        "b"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[ 0.02531881  0.00981773 -0.00932112 -0.03679708]], shape=(1, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 4), dtype=float32, numpy=\n",
              "array([[[ 0.02531881,  0.00981773, -0.00932112, -0.03679708]],\n",
              "\n",
              "       [[ 0.02531881,  0.00981773, -0.00932112, -0.03679708]],\n",
              "\n",
              "       [[ 0.02531881,  0.00981773, -0.00932112, -0.03679708]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC9Bqk4O8ed0",
        "outputId": "99b43867-543e-47ea-874c-e74c9fc99662"
      },
      "source": [
        "c = tf.random.uniform(shape=(3,1,3))\r\n",
        "print(c)\r\n",
        "tf.concat([b,c],axis=2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0.271338   0.50184345 0.32378507]]\n",
            "\n",
            " [[0.41337597 0.21759903 0.04981697]]\n",
            "\n",
            " [[0.45395207 0.05811441 0.22549927]]], shape=(3, 1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 7), dtype=float32, numpy=\n",
              "array([[[ 0.02531881,  0.00981773, -0.00932112, -0.03679708,\n",
              "          0.271338  ,  0.50184345,  0.32378507]],\n",
              "\n",
              "       [[ 0.02531881,  0.00981773, -0.00932112, -0.03679708,\n",
              "          0.41337597,  0.21759903,  0.04981697]],\n",
              "\n",
              "       [[ 0.02531881,  0.00981773, -0.00932112, -0.03679708,\n",
              "          0.45395207,  0.05811441,  0.22549927]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVV7NiO4gzjO"
      },
      "source": [
        "## Positional encoding\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVvCiGxcfYAU"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaphL4wqAoUD"
      },
      "source": [
        "def create_padding_mask(seq):\r\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n",
        "  \r\n",
        "  # add extra dimensions to add the padding\r\n",
        "  # to the attention logits.\r\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpN9RPwJBOMp"
      },
      "source": [
        "def create_look_ahead_mask(size):\r\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\r\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcZMRRo1io6D"
      },
      "source": [
        "## Scaled dot product attention\r\n",
        "\r\n",
        "$${Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y610lR3ioF_"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\r\n",
        "  \"\"\"Calculate the attention weights.\r\n",
        "  q, k, v must have matching leading dimensions.\r\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\r\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \r\n",
        "  but it must be broadcastable for addition.\r\n",
        "  \r\n",
        "  Args:\r\n",
        "    q: query shape == (..., seq_len_q, depth)\r\n",
        "    k: key shape == (..., seq_len_k, depth)\r\n",
        "    v: value shape == (..., seq_len_v, depth_v)\r\n",
        "    mask: Float tensor with shape broadcastable \r\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\r\n",
        "    \r\n",
        "  Returns:\r\n",
        "    output, attention_weights\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\r\n",
        "  \r\n",
        "  # scale matmul_qk\r\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\r\n",
        "\r\n",
        "  # add the mask to the scaled tensor.\r\n",
        "  if mask is not None:\r\n",
        "    scaled_attention_logits += (mask * -1e9)  \r\n",
        "\r\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\r\n",
        "  # add up to 1.\r\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\r\n",
        "\r\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\r\n",
        "\r\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ0MXXUtwoNC"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE5aeVepwQ9Y"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads):\r\n",
        "    super(MultiHeadAttention, self).__init__()\r\n",
        "    self.num_heads = num_heads\r\n",
        "    self.d_model = d_model\r\n",
        "    \r\n",
        "    assert d_model % self.num_heads == 0\r\n",
        "    \r\n",
        "    self.depth = d_model // self.num_heads\r\n",
        "    \r\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\r\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\r\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\r\n",
        "    \r\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\r\n",
        "        \r\n",
        "  def split_heads(self, x, batch_size):\r\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\r\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\r\n",
        "    \"\"\"\r\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\r\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\r\n",
        "    \r\n",
        "  def call(self, v, k, q, mask):\r\n",
        "    batch_size = tf.shape(q)[0]\r\n",
        "    \r\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\r\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\r\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\r\n",
        "    \r\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\r\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\r\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\r\n",
        "    \r\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\r\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\r\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\r\n",
        "        q, k, v, mask)\r\n",
        "    \r\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\r\n",
        "\r\n",
        "    concat_attention = tf.reshape(scaled_attention, \r\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\r\n",
        "\r\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\r\n",
        "        \r\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN2ke3vzyxcf"
      },
      "source": [
        "## Point-wise feed-forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmmKYhGpy3or"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\r\n",
        "  return tf.keras.Sequential([\r\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\r\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\r\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzoSSIvhzB1n"
      },
      "source": [
        "## Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yST4UMS5zqN7"
      },
      "source": [
        "### Encoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn9nNxzMzFd2"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\r\n",
        "    super(EncoderLayer, self).__init__()\r\n",
        "\r\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\r\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n",
        "\r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    \r\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\r\n",
        "    \r\n",
        "  def call(self, x, training, mask):\r\n",
        "\r\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\r\n",
        "    attn_output = self.dropout1(attn_output, training=training)\r\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\r\n",
        "    \r\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\r\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\r\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\r\n",
        "    \r\n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdkZiqi0zvEJ"
      },
      "source": [
        "### Decoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm-3sYRmzgMf"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\r\n",
        "    super(DecoderLayer, self).__init__()\r\n",
        "\r\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\r\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\r\n",
        "\r\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n",
        " \r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    \r\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\r\n",
        "    \r\n",
        "    \r\n",
        "  def call(self, x, enc_output, training, \r\n",
        "           look_ahead_mask, padding_mask):\r\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\r\n",
        "\r\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\r\n",
        "    attn1 = self.dropout1(attn1, training=training)\r\n",
        "    out1 = self.layernorm1(attn1 + x)\r\n",
        "    \r\n",
        "    attn2, attn_weights_block2 = self.mha2(\r\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\r\n",
        "    attn2 = self.dropout2(attn2, training=training)\r\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\r\n",
        "    \r\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\r\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\r\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\r\n",
        "    \r\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiHf_vhwzyX2"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Ss41Tcz03m"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, action_size, rate=0.1):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.num_layers = num_layers\r\n",
        "    \r\n",
        "    self.embedding = tf.keras.layers.Embedding(action_size, d_model)\r\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \r\n",
        "                       for _ in range(num_layers)]\r\n",
        "  \r\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\r\n",
        "        \r\n",
        "  def call(self, x, training, mask):\r\n",
        "    \r\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\r\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "\r\n",
        "    x = self.dropout(x, training=training)\r\n",
        "    \r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x = self.enc_layers[i](x, training, mask)\r\n",
        "    \r\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67TzuRNuz4HL"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wElkmq8tz3Ww"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, action_size, rate=0.1):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.num_layers = num_layers\r\n",
        "    \r\n",
        "    self.embedding = tf.keras.layers.Embedding(action_size, d_model)\r\n",
        "    self.time_embedding = tf.keras.layers.Embedding(61, 4)\r\n",
        "    self.fc = tf.keras.layers.Dense(d_model)\r\n",
        "    \r\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \r\n",
        "                       for _ in range(num_layers)]\r\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\r\n",
        "    \r\n",
        "  def call(self, x, t, enc_output, training, \r\n",
        "           look_ahead_mask, padding_mask):\r\n",
        "\r\n",
        "    attention_weights = {}\r\n",
        "    batch_size = x.shape[0]\r\n",
        "    print(x)\r\n",
        "\r\n",
        "    ut = self.time_embedding(tf.repeat([ts], batch_size, axis=0))  \r\n",
        "\r\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\r\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "\r\n",
        "    x = tf.concat([x, ut], axis=2)     \r\n",
        "    \r\n",
        "    x = self.dropout(x, training=training)\r\n",
        "\r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\r\n",
        "                                             look_ahead_mask, padding_mask)\r\n",
        "      \r\n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\r\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\r\n",
        "    \r\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\r\n",
        "    return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aOGVSoF0POT"
      },
      "source": [
        "## Create the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZnRQTXf0V5_"
      },
      "source": [
        "class Transformer(tf.keras.Model):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, action_size, rate=0.1):\r\n",
        "    super(Transformer, self).__init__()\r\n",
        "\r\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, action_size, rate)\r\n",
        "\r\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, action_size, rate)\r\n",
        "\r\n",
        "    self.final_layer = tf.keras.layers.Dense(action_size)\r\n",
        "    \r\n",
        "  def call(self, inp, tar, t, training, enc_padding_mask, \r\n",
        "           look_ahead_mask, dec_padding_mask):\r\n",
        "\r\n",
        "    print(inp,tar)\r\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\r\n",
        "    \r\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\r\n",
        "    dec_output, attention_weights = self.decoder(\r\n",
        "        tar, t, enc_output, training, look_ahead_mask, dec_padding_mask)\r\n",
        "    \r\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\r\n",
        "    \r\n",
        "    return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCIzvHNwCEE0"
      },
      "source": [
        "## Set hyperparameters\r\n",
        "The time, as a scalar, will be encoded as a time vector of the time through an embedding layer. Max index of time is 59"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqMwBrWLCIzZ"
      },
      "source": [
        "num_layers = 4\r\n",
        "d_model = 128\r\n",
        "dff = 512\r\n",
        "num_heads = 8\r\n",
        "\r\n",
        "action_size = 48\r\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH7yRqee1SgM"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghxLIyPc1YxQ"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
        "  def __init__(self, d_model, warmup_steps=4000):\r\n",
        "    super(CustomSchedule, self).__init__()\r\n",
        "    \r\n",
        "    self.d_model = d_model\r\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\r\n",
        "\r\n",
        "    self.warmup_steps = warmup_steps\r\n",
        "    \r\n",
        "  def __call__(self, step):\r\n",
        "    arg1 = tf.math.rsqrt(step)\r\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\r\n",
        "    \r\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXAIbgfw1huE"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \r\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SanHbKcg1WJY"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiolKdYD1za0"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Q1KINR4Agw"
      },
      "source": [
        "def loss_function(real, pred):\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  loss_ = loss_object(real, pred)\r\n",
        "\r\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
        "  loss_ *= mask\r\n",
        "  \r\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\r\n",
        "\r\n",
        "\r\n",
        "def accuracy_function(real, pred):\r\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\r\n",
        "  \r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\r\n",
        "\r\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\r\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\r\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdjr1fUG4EGu"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubu8U-QH4OQ0"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVNTuCbo4Nre"
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff, action_size, rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBdRfm_VjiPY"
      },
      "source": [
        "def create_masks(inp, tar):\r\n",
        "  # Encoder padding mask\r\n",
        "  enc_padding_mask = create_padding_mask(inp)\r\n",
        "  \r\n",
        "  # Used in the 2nd attention block in the decoder.\r\n",
        "  # This padding mask is used to mask the encoder outputs.\r\n",
        "  dec_padding_mask = create_padding_mask(inp)\r\n",
        "  \r\n",
        "  # Used in the 1st attention block in the decoder.\r\n",
        "  # It is used to pad and mask future tokens in the input received by \r\n",
        "  # the decoder.\r\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\r\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\r\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\r\n",
        "  \r\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbxbjwps4qk_"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/FurtherStudy/Project2020/checkpoints/train_breakfast\"\r\n",
        "\r\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\r\n",
        "                           optimizer=optimizer)\r\n",
        "\r\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n",
        "\r\n",
        "# if a checkpoint exists, restore the latest checkpoint.\r\n",
        "if ckpt_manager.latest_checkpoint:\r\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV7MtkqH5DO2"
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qxx_1wi5C5x"
      },
      "source": [
        "train_step_signature = [\r\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n",
        "    tf.TensorSpec(shape=(None, ), dtype=tf.int64),\r\n",
        "]\r\n",
        "\r\n",
        "@tf.function(input_signature=train_step_signature)\r\n",
        "def train_step(inp, tar, t):\r\n",
        "  print(inp,tar)\r\n",
        "\r\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar)\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    predictions, _ = transformer(inp, tar, t, True, \r\n",
        "                                 enc_padding_mask, \r\n",
        "                                 combined_mask, \r\n",
        "                                 dec_padding_mask)\r\n",
        "    loss = loss_function(tar, predictions)\r\n",
        "\r\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \r\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n",
        "  \r\n",
        "  train_loss(loss)\r\n",
        "  train_accuracy(accuracy_function(tar, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vfB5Qe15Sg9",
        "outputId": "67927d39-39d7-4775-c906-e7a7a97981d5"
      },
      "source": [
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "  \r\n",
        "  train_loss.reset_states()\r\n",
        "  train_accuracy.reset_states()\r\n",
        "  \r\n",
        "  for (batch, (inp, tar, expl_type, expl)) in enumerate(train_dataset):\r\n",
        "    tar = tf.cast(tar, dtype=tf.int64)\r\n",
        "    train_step(inp, tar, t=tf.constant(ts,dtype=tf.int64))\r\n",
        "    \r\n",
        "    if batch % 50 == 0:\r\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\r\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\r\n",
        "      \r\n",
        "  if (epoch + 1) % 5 == 0:\r\n",
        "    ckpt_save_path = ckpt_manager.save()\r\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\r\n",
        "                                                         ckpt_save_path))\r\n",
        "    \r\n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \r\n",
        "                                                train_loss.result(), \r\n",
        "                                                train_accuracy.result()))\r\n",
        "\r\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.4977 Accuracy 0.0118\n",
            "Epoch 1 Batch 50 Loss 3.4943 Accuracy 0.2145\n",
            "Epoch 1 Batch 100 Loss 2.6106 Accuracy 0.4092\n",
            "Epoch 1 Batch 150 Loss 2.0963 Accuracy 0.5141\n",
            "Epoch 1 Batch 200 Loss 1.7458 Accuracy 0.5889\n",
            "Epoch 1 Batch 250 Loss 1.4633 Accuracy 0.6550\n",
            "Epoch 1 Loss 1.3869 Accuracy 0.6732\n",
            "Time taken for 1 epoch: 133.79294443130493 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.1052 Accuracy 0.9807\n",
            "Epoch 2 Batch 50 Loss 0.0964 Accuracy 0.9825\n",
            "Epoch 2 Batch 100 Loss 0.0711 Accuracy 0.9878\n",
            "Epoch 2 Batch 150 Loss 0.0551 Accuracy 0.9907\n",
            "Epoch 2 Batch 200 Loss 0.0440 Accuracy 0.9928\n",
            "Epoch 2 Batch 250 Loss 0.0366 Accuracy 0.9941\n",
            "Epoch 2 Loss 0.0347 Accuracy 0.9945\n",
            "Time taken for 1 epoch: 130.01863169670105 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0077 Accuracy 0.9981\n",
            "Epoch 3 Batch 50 Loss 0.0036 Accuracy 0.9996\n",
            "Epoch 3 Batch 100 Loss 0.0027 Accuracy 0.9997\n",
            "Epoch 3 Batch 150 Loss 0.0022 Accuracy 0.9998\n",
            "Epoch 3 Batch 200 Loss 0.0023 Accuracy 0.9998\n",
            "Epoch 3 Batch 250 Loss 0.0022 Accuracy 0.9998\n",
            "Epoch 3 Loss 0.0021 Accuracy 0.9998\n",
            "Time taken for 1 epoch: 128.96903324127197 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0006 Accuracy 1.0000\n",
            "Epoch 4 Batch 50 Loss 0.0003 Accuracy 1.0000\n",
            "Epoch 4 Batch 100 Loss 0.0002 Accuracy 1.0000\n",
            "Epoch 4 Batch 150 Loss 0.0002 Accuracy 1.0000\n",
            "Epoch 4 Batch 200 Loss 0.0002 Accuracy 1.0000\n",
            "Epoch 4 Batch 250 Loss 0.0005 Accuracy 0.9999\n",
            "Epoch 4 Loss 0.0005 Accuracy 0.9999\n",
            "Time taken for 1 epoch: 127.6217794418335 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0002 Accuracy 1.0000\n",
            "Epoch 5 Batch 50 Loss 0.0002 Accuracy 1.0000\n",
            "Epoch 5 Batch 100 Loss 0.0001 Accuracy 1.0000\n",
            "Epoch 5 Batch 150 Loss 0.0001 Accuracy 1.0000\n",
            "Epoch 5 Batch 200 Loss 0.0001 Accuracy 1.0000\n",
            "Epoch 5 Batch 250 Loss 0.0001 Accuracy 1.0000\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/FurtherStudy/Project2020/checkpoints/train_breakfast/ckpt-1\n",
            "Epoch 5 Loss 0.0001 Accuracy 1.0000\n",
            "Time taken for 1 epoch: 131.87099289894104 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 6 Batch 50 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 6 Batch 100 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 6 Batch 150 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 6 Batch 200 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 6 Batch 250 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 6 Loss 0.0000 Accuracy 1.0000\n",
            "Time taken for 1 epoch: 128.56895351409912 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 7 Batch 50 Loss 0.0031 Accuracy 0.9991\n",
            "Epoch 7 Batch 100 Loss 0.0018 Accuracy 0.9995\n",
            "Epoch 7 Batch 150 Loss 0.0012 Accuracy 0.9997\n",
            "Epoch 7 Batch 200 Loss 0.0009 Accuracy 0.9997\n",
            "Epoch 7 Batch 250 Loss 0.0008 Accuracy 0.9998\n",
            "Epoch 7 Loss 0.0007 Accuracy 0.9998\n",
            "Time taken for 1 epoch: 129.60865950584412 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 8 Batch 50 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 8 Batch 100 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 8 Batch 150 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 8 Batch 200 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 8 Batch 250 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 8 Loss 0.0000 Accuracy 1.0000\n",
            "Time taken for 1 epoch: 130.1188313961029 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 9 Batch 50 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 9 Batch 100 Loss 0.0042 Accuracy 0.9991\n",
            "Epoch 9 Batch 150 Loss 0.0053 Accuracy 0.9987\n",
            "Epoch 9 Batch 200 Loss 0.0042 Accuracy 0.9990\n",
            "Epoch 9 Batch 250 Loss 0.0036 Accuracy 0.9992\n",
            "Epoch 9 Loss 0.0035 Accuracy 0.9992\n",
            "Time taken for 1 epoch: 128.55587196350098 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0005 Accuracy 1.0000\n",
            "Epoch 10 Batch 50 Loss 0.0017 Accuracy 0.9996\n",
            "Epoch 10 Batch 100 Loss 0.0010 Accuracy 0.9997\n",
            "Epoch 10 Batch 150 Loss 0.0007 Accuracy 0.9998\n",
            "Epoch 10 Batch 200 Loss 0.0005 Accuracy 0.9999\n",
            "Epoch 10 Batch 250 Loss 0.0013 Accuracy 0.9996\n",
            "Saving checkpoint for epoch 10 at /content/drive/MyDrive/FurtherStudy/Project2020/checkpoints/train_breakfast/ckpt-2\n",
            "Epoch 10 Loss 0.0017 Accuracy 0.9995\n",
            "Time taken for 1 epoch: 129.8141348361969 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0003 Accuracy 1.0000\n",
            "Epoch 11 Batch 50 Loss 0.0014 Accuracy 0.9998\n",
            "Epoch 11 Batch 100 Loss 0.0019 Accuracy 0.9996\n",
            "Epoch 11 Batch 150 Loss 0.0019 Accuracy 0.9996\n",
            "Epoch 11 Batch 200 Loss 0.0016 Accuracy 0.9997\n",
            "Epoch 11 Batch 250 Loss 0.0013 Accuracy 0.9997\n",
            "Epoch 11 Loss 0.0012 Accuracy 0.9997\n",
            "Time taken for 1 epoch: 130.4513397216797 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 12 Batch 50 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 12 Batch 100 Loss 0.0177 Accuracy 0.9956\n",
            "Epoch 12 Batch 150 Loss 0.0121 Accuracy 0.9970\n",
            "Epoch 12 Batch 200 Loss 0.0095 Accuracy 0.9977\n",
            "Epoch 12 Batch 250 Loss 0.0076 Accuracy 0.9981\n",
            "Epoch 12 Loss 0.0072 Accuracy 0.9982\n",
            "Time taken for 1 epoch: 130.42722940444946 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 13 Batch 50 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 13 Batch 100 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 13 Batch 150 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 13 Batch 200 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 13 Batch 250 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 13 Loss 0.0000 Accuracy 1.0000\n",
            "Time taken for 1 epoch: 130.94629073143005 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 14 Batch 50 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 14 Batch 100 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 14 Batch 150 Loss 0.0006 Accuracy 0.9999\n",
            "Epoch 14 Batch 200 Loss 0.0051 Accuracy 0.9986\n",
            "Epoch 14 Batch 250 Loss 0.0058 Accuracy 0.9984\n",
            "Epoch 14 Loss 0.0056 Accuracy 0.9985\n",
            "Time taken for 1 epoch: 129.83169412612915 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0001 Accuracy 1.0000\n",
            "Epoch 15 Batch 50 Loss 0.0007 Accuracy 0.9998\n",
            "Epoch 15 Batch 100 Loss 0.0019 Accuracy 0.9995\n",
            "Epoch 15 Batch 150 Loss 0.0065 Accuracy 0.9985\n",
            "Epoch 15 Batch 200 Loss 0.0089 Accuracy 0.9981\n",
            "Epoch 15 Batch 250 Loss 0.0075 Accuracy 0.9984\n",
            "Saving checkpoint for epoch 15 at /content/drive/MyDrive/FurtherStudy/Project2020/checkpoints/train_breakfast/ckpt-3\n",
            "Epoch 15 Loss 0.0072 Accuracy 0.9984\n",
            "Time taken for 1 epoch: 131.1787712574005 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0010 Accuracy 1.0000\n",
            "Epoch 16 Batch 50 Loss 0.0003 Accuracy 1.0000\n",
            "Epoch 16 Batch 100 Loss 0.0051 Accuracy 0.9984\n",
            "Epoch 16 Batch 150 Loss 0.0109 Accuracy 0.9971\n",
            "Epoch 16 Batch 200 Loss 0.0084 Accuracy 0.9978\n",
            "Epoch 16 Batch 250 Loss 0.0068 Accuracy 0.9982\n",
            "Epoch 16 Loss 0.0064 Accuracy 0.9983\n",
            "Time taken for 1 epoch: 130.79374861717224 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 17 Batch 50 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 17 Batch 100 Loss 0.0001 Accuracy 1.0000\n",
            "Epoch 17 Batch 150 Loss 0.0001 Accuracy 1.0000\n",
            "Epoch 17 Batch 200 Loss 0.0001 Accuracy 1.0000\n",
            "Epoch 17 Batch 250 Loss 0.0029 Accuracy 0.9993\n",
            "Epoch 17 Loss 0.0034 Accuracy 0.9992\n",
            "Time taken for 1 epoch: 130.15057349205017 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0022 Accuracy 1.0000\n",
            "Epoch 18 Batch 50 Loss 0.0192 Accuracy 0.9966\n",
            "Epoch 18 Batch 100 Loss 0.0106 Accuracy 0.9980\n",
            "Epoch 18 Batch 150 Loss 0.0078 Accuracy 0.9985\n",
            "Epoch 18 Batch 200 Loss 0.0071 Accuracy 0.9986\n",
            "Epoch 18 Batch 250 Loss 0.0066 Accuracy 0.9987\n",
            "Epoch 18 Loss 0.0071 Accuracy 0.9986\n",
            "Time taken for 1 epoch: 131.2028443813324 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0260 Accuracy 0.9900\n",
            "Epoch 19 Batch 50 Loss 0.0106 Accuracy 0.9976\n",
            "Epoch 19 Batch 100 Loss 0.0070 Accuracy 0.9984\n",
            "Epoch 19 Batch 150 Loss 0.0051 Accuracy 0.9988\n",
            "Epoch 19 Batch 200 Loss 0.0039 Accuracy 0.9991\n",
            "Epoch 19 Batch 250 Loss 0.0032 Accuracy 0.9992\n",
            "Epoch 19 Loss 0.0030 Accuracy 0.9993\n",
            "Time taken for 1 epoch: 131.16311287879944 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 20 Batch 50 Loss 0.0000 Accuracy 1.0000\n",
            "Epoch 20 Batch 100 Loss 0.0001 Accuracy 1.0000\n",
            "Epoch 20 Batch 150 Loss 0.0041 Accuracy 0.9993\n",
            "Epoch 20 Batch 200 Loss 0.0038 Accuracy 0.9993\n",
            "Epoch 20 Batch 250 Loss 0.0031 Accuracy 0.9994\n",
            "Saving checkpoint for epoch 20 at /content/drive/MyDrive/FurtherStudy/Project2020/checkpoints/train_breakfast/ckpt-4\n",
            "Epoch 20 Loss 0.0029 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 132.14399933815002 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N18gdrknSGyp"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/FurtherStudy/Project2020/checkpoints/train_breakfast1\"\r\n",
        "\r\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\r\n",
        "                           optimizer=optimizer)\r\n",
        "\r\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n",
        "\r\n",
        "# if a checkpoint exists, restore the latest checkpoint.\r\n",
        "if ckpt_manager.latest_checkpoint:\r\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "id": "MbZhLVixSGmI",
        "outputId": "12262ec8-7330-4884-b409-3c43ab7525b5"
      },
      "source": [
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "  \r\n",
        "  train_loss.reset_states()\r\n",
        "  train_accuracy.reset_states()\r\n",
        "  \r\n",
        "  for (batch, (inp, tar, expl_type, expl)) in enumerate(train_dataset):\r\n",
        "    tar = tf.cast(tar, dtype=tf.int64)\r\n",
        "    train_step(inp, tar, t=tf.constant(ts,dtype=tf.int64))\r\n",
        "    \r\n",
        "    if batch % 50 == 0:\r\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\r\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\r\n",
        "      \r\n",
        "  if (epoch + 1) % 5 == 0:\r\n",
        "    ckpt_save_path = ckpt_manager.save()\r\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\r\n",
        "                                                         ckpt_save_path))\r\n",
        "    \r\n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \r\n",
        "                                                train_loss.result(), \r\n",
        "                                                train_accuracy.result()))\r\n",
        "\r\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"inp:0\", shape=(None, None), dtype=int64) Tensor(\"tar:0\", shape=(None, None), dtype=int64)\n",
            "Tensor(\"inp:0\", shape=(None, None), dtype=int64) Tensor(\"tar:0\", shape=(None, None), dtype=int64)\n",
            "Tensor(\"tar:0\", shape=(None, None), dtype=int64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-d961e8e3afbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-74-e4eee10f0656>:14 train_step  *\n        predictions, _ = transformer(inp, tar, t, True,\n    <ipython-input-70-e7df7da48649>:18 call  *\n        dec_output, attention_weights = self.decoder(\n    <ipython-input-60-97655dc7dbca>:23 call  *\n        ut = self.time_embedding(tf.repeat([ts], batch_size, axis=0))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:6336 repeat\n        return repeat_with_axis(input, repeats, axis, name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:6184 repeat_with_axis\n        repeats = convert_to_int_tensor(repeats, name=\"repeats\")\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:6090 convert_to_int_tensor\n        tensor = ops.convert_to_tensor(tensor, name=name, preferred_dtype=dtype)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:265 constant\n        allow_broadcast=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:283 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:445 make_tensor_proto\n        raise ValueError(\"None values not supported.\")\n\n    ValueError: None values not supported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP9cQvrxTSzN",
        "outputId": "ce8b0662-552a-40b6-8ea8-a2b62e9d086c"
      },
      "source": [
        "tf.repeat([ts], 4, axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 10), dtype=int32, numpy=\n",
              "array([[ 1,  3,  5,  7, 10, 20, 30, 40, 50, 60],\n",
              "       [ 1,  3,  5,  7, 10, 20, 30, 40, 50, 60],\n",
              "       [ 1,  3,  5,  7, 10, 20, 30, 40, 50, 60],\n",
              "       [ 1,  3,  5,  7, 10, 20, 30, 40, 50, 60]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qHNInX_uJ25"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eca0yt0tuN9F"
      },
      "source": [
        "def evaluate(inp_sentence):\r\n",
        "  start_token = [tokenizer_pt.vocab_size]\r\n",
        "  end_token = [tokenizer_pt.vocab_size + 1]\r\n",
        "  \r\n",
        "  # inp sentence is portuguese, hence adding the start and end token\r\n",
        "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\r\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\r\n",
        "  \r\n",
        "  # as the target is english, the first word to the transformer should be the\r\n",
        "  # english start token.\r\n",
        "  decoder_input = [tokenizer_en.vocab_size]\r\n",
        "  output = tf.expand_dims(decoder_input, 0)\r\n",
        "    \r\n",
        "  for i in range(MAX_LENGTH):\r\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\r\n",
        "        encoder_input, output)\r\n",
        "  \r\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\r\n",
        "    predictions, attention_weights = transformer(encoder_input, \r\n",
        "                                                 output,\r\n",
        "                                                 False,\r\n",
        "                                                 enc_padding_mask,\r\n",
        "                                                 combined_mask,\r\n",
        "                                                 dec_padding_mask)\r\n",
        "    \r\n",
        "    # select the last word from the seq_len dimension\r\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\r\n",
        "\r\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\r\n",
        "    \r\n",
        "    # return the result if the predicted_id is equal to the end token\r\n",
        "    if predicted_id == tokenizer_en.vocab_size+1:\r\n",
        "      return tf.squeeze(output, axis=0), attention_weights\r\n",
        "    \r\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\r\n",
        "    # as its input.\r\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\r\n",
        "\r\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkM-59R0uTEx"
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\r\n",
        "  fig = plt.figure(figsize=(16, 8))\r\n",
        "  \r\n",
        "  sentence = tokenizer_pt.encode(sentence)\r\n",
        "  \r\n",
        "  attention = tf.squeeze(attention[layer], axis=0)\r\n",
        "  \r\n",
        "  for head in range(attention.shape[0]):\r\n",
        "    ax = fig.add_subplot(2, 4, head+1)\r\n",
        "    \r\n",
        "    # plot the attention weights\r\n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\r\n",
        "\r\n",
        "    fontdict = {'fontsize': 10}\r\n",
        "    \r\n",
        "    ax.set_xticks(range(len(sentence)+2))\r\n",
        "    ax.set_yticks(range(len(result)))\r\n",
        "    \r\n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\r\n",
        "        \r\n",
        "    ax.set_xticklabels(\r\n",
        "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \r\n",
        "        fontdict=fontdict, rotation=90)\r\n",
        "    \r\n",
        "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \r\n",
        "                        if i < tokenizer_en.vocab_size], \r\n",
        "                       fontdict=fontdict)\r\n",
        "    \r\n",
        "    ax.set_xlabel('Head {}'.format(head+1))\r\n",
        "  \r\n",
        "  plt.tight_layout()\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50hEKldxuWol"
      },
      "source": [
        "def anticipate(obs):\r\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zspFbVsaEgQ"
      },
      "source": [
        "## Encode the actions by tokenizer and use word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaNF_1X7aD81"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAyb63Knn0En"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-bdkL6Kkemk",
        "outputId": "609eebd0-25b9-4df7-b60d-700d992a2dd3"
      },
      "source": [
        "from sklearn.model_selection import KFold\r\n",
        "cv = KFold(n_splits=4)\r\n",
        "cv.get_n_splits()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHTjftrRnzps",
        "outputId": "9a47f905-93d6-4232-db1e-196e4f80f41a"
      },
      "source": [
        "action_encode('take_topping')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x6re_ZMUn-uo",
        "outputId": "1b90c321-4c89-4e7b-8144-1642538e9e9e"
      },
      "source": [
        "action_decode(np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pour_water'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmUoWABxxKYu",
        "outputId": "848f4f39-ab30-4427-9467-71dbd4a6422b"
      },
      "source": [
        "testv = VideoStream2(\"/content/drive/MyDrive/FurtherStudy/Project2020/groundTruthlabel-breakfastdatset/\", \"P03_cam01_P03_cereals.txt\")\r\n",
        "testobs = testv.obs_by_frame_index(end=450, start=0)\r\n",
        "testfm = build_features(testobs)\r\n",
        "print(testfm)\r\n",
        "np.sum(testfm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgJj6LOBxMpZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FSpmHSor-QA",
        "outputId": "1e27d414-6c80-460c-e169-524fe09bc1cd"
      },
      "source": [
        "a=tf.random.uniform(shape=(5,2))\r\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
              "array([[0.4869156 , 0.50380003],\n",
              "       [0.40671933, 0.9091934 ],\n",
              "       [0.58814025, 0.16146433],\n",
              "       [0.96441555, 0.98910534],\n",
              "       [0.38232553, 0.444448  ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gJkShjUvwYa",
        "outputId": "33d2e777-30dc-4fd0-e281-4d9a0a65c5ee"
      },
      "source": [
        "b=tf.random.uniform(shape=(4,5,3))\r\n",
        "b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 5, 3), dtype=float32, numpy=\n",
              "array([[[0.369779  , 0.47258306, 0.3311255 ],\n",
              "        [0.11288857, 0.14152324, 0.93651605],\n",
              "        [0.10874283, 0.7000512 , 0.19770205],\n",
              "        [0.6681651 , 0.8744607 , 0.52170336],\n",
              "        [0.96739507, 0.1843214 , 0.4352312 ]],\n",
              "\n",
              "       [[0.7064192 , 0.8644029 , 0.31154227],\n",
              "        [0.80514944, 0.0209229 , 0.15270925],\n",
              "        [0.37945592, 0.11959791, 0.8032012 ],\n",
              "        [0.41443884, 0.4430169 , 0.32570255],\n",
              "        [0.34442377, 0.6477808 , 0.47210932]],\n",
              "\n",
              "       [[0.74103355, 0.29591465, 0.07663846],\n",
              "        [0.28255117, 0.24936521, 0.03243327],\n",
              "        [0.45256853, 0.83092606, 0.10536003],\n",
              "        [0.24437308, 0.06488681, 0.96194375],\n",
              "        [0.69624734, 0.7934059 , 0.6293143 ]],\n",
              "\n",
              "       [[0.8969368 , 0.44867074, 0.73157835],\n",
              "        [0.32141197, 0.90469885, 0.34563053],\n",
              "        [0.62794554, 0.96432805, 0.03058159],\n",
              "        [0.26197994, 0.61175466, 0.1110425 ],\n",
              "        [0.32369578, 0.5314319 , 0.74524856]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py186DPJs1Ue",
        "outputId": "296b435b-4c36-484b-af50-6a9fb7637b17"
      },
      "source": [
        "c=tf.repeat([a],4,axis=0)\r\n",
        "c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 5, 2), dtype=float32, numpy=\n",
              "array([[[0.4869156 , 0.50380003],\n",
              "        [0.40671933, 0.9091934 ],\n",
              "        [0.58814025, 0.16146433],\n",
              "        [0.96441555, 0.98910534],\n",
              "        [0.38232553, 0.444448  ]],\n",
              "\n",
              "       [[0.4869156 , 0.50380003],\n",
              "        [0.40671933, 0.9091934 ],\n",
              "        [0.58814025, 0.16146433],\n",
              "        [0.96441555, 0.98910534],\n",
              "        [0.38232553, 0.444448  ]],\n",
              "\n",
              "       [[0.4869156 , 0.50380003],\n",
              "        [0.40671933, 0.9091934 ],\n",
              "        [0.58814025, 0.16146433],\n",
              "        [0.96441555, 0.98910534],\n",
              "        [0.38232553, 0.444448  ]],\n",
              "\n",
              "       [[0.4869156 , 0.50380003],\n",
              "        [0.40671933, 0.9091934 ],\n",
              "        [0.58814025, 0.16146433],\n",
              "        [0.96441555, 0.98910534],\n",
              "        [0.38232553, 0.444448  ]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovizFG5uwAQ_",
        "outputId": "d9aa3548-be46-43ec-9923-5d76ba031875"
      },
      "source": [
        "tf.concat([b,c],axis=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 5, 5), dtype=float32, numpy=\n",
              "array([[[0.369779  , 0.47258306, 0.3311255 , 0.4869156 , 0.50380003],\n",
              "        [0.11288857, 0.14152324, 0.93651605, 0.40671933, 0.9091934 ],\n",
              "        [0.10874283, 0.7000512 , 0.19770205, 0.58814025, 0.16146433],\n",
              "        [0.6681651 , 0.8744607 , 0.52170336, 0.96441555, 0.98910534],\n",
              "        [0.96739507, 0.1843214 , 0.4352312 , 0.38232553, 0.444448  ]],\n",
              "\n",
              "       [[0.7064192 , 0.8644029 , 0.31154227, 0.4869156 , 0.50380003],\n",
              "        [0.80514944, 0.0209229 , 0.15270925, 0.40671933, 0.9091934 ],\n",
              "        [0.37945592, 0.11959791, 0.8032012 , 0.58814025, 0.16146433],\n",
              "        [0.41443884, 0.4430169 , 0.32570255, 0.96441555, 0.98910534],\n",
              "        [0.34442377, 0.6477808 , 0.47210932, 0.38232553, 0.444448  ]],\n",
              "\n",
              "       [[0.74103355, 0.29591465, 0.07663846, 0.4869156 , 0.50380003],\n",
              "        [0.28255117, 0.24936521, 0.03243327, 0.40671933, 0.9091934 ],\n",
              "        [0.45256853, 0.83092606, 0.10536003, 0.58814025, 0.16146433],\n",
              "        [0.24437308, 0.06488681, 0.96194375, 0.96441555, 0.98910534],\n",
              "        [0.69624734, 0.7934059 , 0.6293143 , 0.38232553, 0.444448  ]],\n",
              "\n",
              "       [[0.8969368 , 0.44867074, 0.73157835, 0.4869156 , 0.50380003],\n",
              "        [0.32141197, 0.90469885, 0.34563053, 0.40671933, 0.9091934 ],\n",
              "        [0.62794554, 0.96432805, 0.03058159, 0.58814025, 0.16146433],\n",
              "        [0.26197994, 0.61175466, 0.1110425 , 0.96441555, 0.98910534],\n",
              "        [0.32369578, 0.5314319 , 0.74524856, 0.38232553, 0.444448  ]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}